  
#pragma kernel CreateImageGeoConeMirror
#pragma kernel CreateImageParaboloidMirror

#pragma kernel CreateImageHemisphereMirror


//#pragma kernel CreateImageHemisphereMirror_RT
//
//kernelTriConeMirror = RayTracingShader.FindKernel("TriConeMirror");
//kernelGeoConeMirror = RayTracingShader.FindKernel("GeoConeMirror");
//kernelParaboloidMirror = RayTracingShader.FindKernel("ParaboloidMirror");

//https://forum.unity.com/threads/terrain-composer2-a-next-generation-gpu-powered-terrain-tool.151365/page-126
//In the 2018.1 release notes they mention this change:
//Shaders : ComputeShader.Dispatch now validates if all the properties are valid.
//So Unity will not run compute shaders if there are properties not set before running Dispatch

//SetComputeBuffer("_IntersectionBuffer", mIntersectionBuffer);
//SetComputeBuffer("_AccumRayEnergyBuffer", mAccumRayEnergyBuffer);
//SetComputeBuffer("_EmissionBuffer", mEmissionBuffer);
//SetComputeBuffer("_SpecularBuffer", mSpecularBuffer);

//_AccumRayEnergyBuffer
//RWStructuredBuffer<float4> _RayDirectionBuffer;
//RWStructuredBuffer<float4> _IntersectionBuffer;
//RWStructuredBuffer<float4> _AccumRayEnergyBuffer;
//RWStructuredBuffer<float4> _EmissionBuffer;
//RWStructuredBuffer<float4> _SpecularBuffer;

//
// Undistortion structs
//
struct CameraParams
{
	float3 RadialCoefficient;
	float2 TangentialCoefficient;
	float2 PrincipalPoint;
	float2 FocalLength;
	float SkewCoefficient;
};

int _CurrentCounter = 0;
int _SafeCounter;
float4 _ThresholdIterative;
int _UndistortMode;
int _MirrorMode; // 0= Cone, 1 = Parabola, 2 = Hemisphere, 
float4 _ThresholdNewton;

StructuredBuffer<CameraParams> _CameraLensDistortionParams;

float2 get_undistorted_ndc_newton(float2 p_d, in CameraParams cameraParams, float2 thresholdNewton /* = 0.01 */);
//
//
//
float2 get_undistorted_ndc_iterative(float2 p_d, in CameraParams cameraParams, float2 thresholdIterative /* = 0.01*/);
//
//
//
float2 get_undistorted_ndc_direct(float2 p_d, in CameraParams cameraParams);
//
//
//
float2 normalize(float x_u, float y_u, in CameraParams cameraParams);
//
//
//
float2 denormalize(float x_u, float y_u, in CameraParams cameraParams);
//
//
//
float2 distort_normalized(float x_nu, float y_nu, in CameraParams cameraParams);
//-------------------------------------
//- MESHES


struct MeshObject
{
	float4x4 localToWorldMatrix;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	int indices_offset;
	int indices_count;
};

StructuredBuffer<MeshObject> _MeshObjects;


struct TriangularConeMirror
{
	float4x4 localToWorldMatrix;

	float distanceToOrigin;
	float height;
	float radius;

	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	int indices_offset;
	int indices_count;
};


StructuredBuffer<TriangularConeMirror> _TriangularConeMirrors;

struct HemisphereMirror
{
	float4x4 localToWorldMatrix;

	float distanceToOrigin;
	float height;
	float usedHeight;
	float bottomDiscRadius;
	float sphereRadius;
	float notUsedHeightRatio;

	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	//int indices_offset;
	//int indices_count;

};

StructuredBuffer<HemisphereMirror> _HemisphereMirrors;

//-PYRAMID MIRROR------------------------------------
struct PyramidMirror
{
	float4x4 localToWorldMatrix; // the world frame of the pyramid
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	float height;
	float width; // the radius of the base of the cone
	float depth;
};


StructuredBuffer<PyramidMirror> _PyramidMirrors;
//- CONE

struct GeoConeMirror
{
	float4x4 localToWorldMatrix; // the world frame of the cone
	float distanceToOrigin;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	float height;
	float radius; // the radius of the base of the cone

};
//- CONE
StructuredBuffer<GeoConeMirror> _geoConeMirrors;

struct EllipsoidMirror
{
	float4x4 localToWorldMatrix; // the  frame of the cone
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	float radiusA; // the radius along the x axis
	float radiusB; // the radius along the y axis
};
//- CONE


StructuredBuffer<EllipsoidMirror> _EllipsoidMirrors;

struct ParaboloidMirror
{
	float4x4 localToWorldMatrix; // the frame of the paraboloid
	float distanceToOrigin; // distance from the camera to the curPixelin of the paraboloid
	float height;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	float coefficientA; // z = - ( x^2/a^2 + y^2/b^2)
	float coefficientB;
};
//- CONE


StructuredBuffer<ParaboloidMirror> _ParaboloidMirrors;


struct PanoramaScreen
{
	float4x4 localToWorldMatrix;
	float highRange;
	float lowRange;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;

};

StructuredBuffer<PanoramaScreen> _PanoramaScreens;


struct PanoramaMesh
{
	float4x4 localToWorldMatrix;
	float highRange;
	float lowRange;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
	int indices_offset;
	int indices_count;
};

StructuredBuffer<PanoramaMesh> _PanoramaMeshes;

//- SPHERES

struct Sphere
{
	float3 position;
	float radius;
	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
};

StructuredBuffer<Sphere> _Spheres;


StructuredBuffer<float3> _Vertices;
StructuredBuffer<int> _Indices;
StructuredBuffer<float2> _UVs;

RWStructuredBuffer<float3> _VertexBufferRW;


//float3x2 _VtxUVs; commented out by Moon Jung, 2020/1/21

RWTexture2D<float4> _DebugRWTexture;

RWTexture2D<float4> _Result; // To store the result of raytracing; 
// first of all, it is used to store the predistorted image


float4x4 _CameraToWorld;
float3 _CameraViewDirection;
float4   _CameraForwardDirection; // the Z axis of the Camera

float4x4 _Projection;
float4x4 _CameraInverseProjection;


float4 _DirectionalLight;

float4 _PixelOffset;

Texture2D<float4> _SkyboxTexture;
SamplerState sampler_SkyboxTexture;

int _NumOfTargetTextures;

Texture2D<float4> _RoomTexture0, _RoomTexture1, _RoomTexture2, _RoomTexture3;
SamplerState sampler_RoomTexture0, sampler_RoomTexture1, sampler_RoomTexture2, sampler_RoomTexture3;



float _FOV; // in radian added by Moon
int _MaxBounce;

static const float PI = 3.14159265f;
static const float EPSILON = 1e-8;

//-------------------------------------
//- UTILITY

float sdot(float3 x, float3 y, float f = 1.0f)
{
	return saturate(dot(x, y) * f);
}

float energy(float3 color)
{
	return dot(color, 1.0f / 3.0f);
}

//-------------------------------------
//- RANDOMNESS

float2 _Pixel;
float _Seed;

float rand()
{
	float result = frac(sin(_Seed / 100.0f * dot(_Pixel, float2(12.9898f, 78.233f))) * 43758.5453f);
	_Seed += 1.0f;
	return result;
}


//-------------------------------------
//- RAY

struct Ray
{
	float3 origin;
	float3 direction;
	float3 localDirectionInCamera;
	float3 energy;
};

Ray CreateRay(float3 origin, float3 direction, float3 localDirectionInCamera)
{
	Ray ray;
	ray.origin = origin;
	ray.direction = direction;
	ray.localDirectionInCamera = localDirectionInCamera;
	ray.energy = float3(1.0f, 1.0f, 1.0f);
	return ray;
}

Ray CreateCameraRay(float2 undistorted_ndc)
{

	// for debugging
	// Get the dimensions of the RenderTexture
	uint width, height;

	_Result.GetDimensions(width, height);

	// Transform the camera origin in OpenGL space to world space in Unity 
	//   RTShader.SetMatrix("_CameraToWorld", MainCamera.cameraToWorldMatrix);
	float3 cameraOriginInWorld = mul(_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;

	// "forward" in OpenGL is "-z".In Unity forward is "+z".Most hand - rules you might know from math are inverted in Unity
	   //    .For example the cross product usually uses the right hand rule c = a x b where a is thumb, b is index finger and c is the middle
	   //    finger.In Unity you would use the same logic, but with the left hand.

	   //    However this does not affect the projection matrix as Unity uses the OpenGL convention for the projection matrix.
	   //    The required z - flipping is done by the cameras worldToCameraMatrix.
	   //    So the projection matrix should look the same as in OpenGL.

	 // Invert  undistorted_ndc to the camera space in OpenGL
	 // undistorted_ndc is the normalized device coordinates ranging from -1 to 1
	//float3 posInOpenGLCamera = mul(_CameraInverseProjection, float4(undistorted_ndc, 0.0f, 1.0f)).xyz;
	// [ xnwn, ynwn, znwn, wn] = _Projection* ( xe, ye, ze, 1)
	// Find (xe,ye,ze,1) corresponding to wn* (xn,yn, (-1), 1)
	float3 posInOpenGLCamera = mul(_CameraInverseProjection, float4(undistorted_ndc, -1.0f, 1.0f)).xyz;
	float3 localDirectionInOpenGLCamera = normalize(posInOpenGLCamera);

	
	//float3 posInScreenSpace = mul(_Projection, float4(posInCamera, 1.0f)).xyz;
	//debugging

	//float3 myPosInCamera = float3(myxyNDC, -1);
	//float3 myPosInScreenSpace = mul(_Projection, float4(myPosInCamera, 1.0f)).xyz;
	//myDir = normalize(myDir);

	// for debugging
	//_IntersectionBuffer[id.y * width + id.x] = float4( normalize( posInCameraZero), 0);
	//_RayDirectionBuffer[id.y * width + id.x] = float4( normalize( posInCameraMinusOne), 0);

	//_EmissionBuffer[id.y * width + id.x] = float4( normalize( myPosInCamera), 0);
	 // _SpecularBuffer[id.y * width + id.x] = float4(myPosInCamera, 0);

	// Transform the direction from camera to world space and normalize
	float3 dirInUnityWorld = mul(_CameraToWorld, float4(posInOpenGLCamera, 0.0f)).xyz;

	float3 globalDirection = normalize(dirInUnityWorld);
	//float3 direction = normalize(myPosInCamera);

	//return CreateRay(curPixelin, myPosInCamera);
	return CreateRay(cameraOriginInWorld, globalDirection, localDirectionInOpenGLCamera);
}


//-------------------------------------
//- RAYHIT

struct RayHit
{
	float3 position; // the hit position
	float2 uvInTriangle; // the barycentric coord of the hit point relative
			   // the sorrounding triangle
	float3x2 vtxUVs; // added by Moon Jung, 2020/1/21

	float distance;
	float3 normal; // the normal at the ray hit point

	float3 albedo;
	float3 specular;
	float smoothness;
	float3 emission;
};

RayHit CreateRayHit()
{
	RayHit hit = (RayHit)0;
	hit.position = float3(0.0f, 0.0f, 0.0f);
	hit.vtxUVs = float3x2(float2(0.0f, 0.0f), float2(0.0f, 0.0f), float2(0.0f, 0.0f));
	hit.distance = 1.#INF;
	hit.normal = float3(0.0f, 0.0f, 0.0f);
	hit.uvInTriangle = float2(0.0f, 0.0f);
	hit.albedo = float3(0.0f, 0.0f, 0.0f);
	hit.specular = float3(0.0f, 0.0f, 0.0f);
	hit.smoothness = 0.0f;
	hit.emission = float3(0.0f, 0.0f, 0.0f);
	return hit;
}

//-------------------------------------
//- INTERSECTION

void IntersectGroundPlane(Ray ray, inout RayHit bestHit)
{
	// Calculate distance along the ray where the ground plane is intersected
	float t = -ray.origin.y / ray.direction.y;
	if (t > 0 && t < bestHit.distance)
	{

		bestHit.distance = t;
		bestHit.position = ray.origin + t * ray.direction;
		bestHit.normal = float3(0.0f, 1.0f, 0.0f);

		bestHit.albedo = 0.5f;
		bestHit.specular = 0.03f;
		bestHit.smoothness = 0.2f;
		bestHit.emission = float3(0.0f, 0.0f, 0.0f);
	}
}

//
// Functions Prototypes
//
bool IntersectTriangle_MT97(Ray ray, float3 vert0, float3 vert1, float3 vert2, out float t, out float u, out float v);

void IntersectPyramidMirror(Ray ray, inout RayHit bestHit, PyramidMirror pyramid);

void IntersectParaboloidMirror(Ray ray, inout RayHit bestHit, ParaboloidMirror paraboloid);

void IntersectConeMirror(Ray ray, inout RayHit bestHit, GeoConeMirror cone)
{
	// Calculate distance along the ray where the cone is intersected
	// equation: Find t, h, theta such that 
	//   ray.dir * t = apex + (h*tan(phi)cos(theta),  h, 
	//                         h*tan(phi)sin(theta) )
	// Note: The coordinate system is the OPENGL coordinate system with
	// y: up, -z: forward, x: right; All coordinates are global
	// tan(phi) = cone.radius / cone/height;

	//  ray.dir.x * t - apex.x = h * (cone.radius/cone.height) cos(theta) (1)
	// ray.dir.z * t - apex.z = h * (cone.radius/cone.height) sin(theta)  (2)
	// 
	// ray.dir.y * t = apex.y + h;  
	// 
	// Obtain a quadratic equation for t from the above three equations
	// Then obtain h and theta
	/*

	float t =
	  float h =
	  float x =
	  float z = ;
	float3 normal = ;

	if (t > 0 && t < bestHit.distance)
	{
	  bestHit.hitSurfaceType = 1; // cone
	  bestHit.distance = t;
	  bestHit.position =ray.origin + t * ray.direction;
	  bestHit.normal = normal;

	  bestHit.albedo = cone.albedo;
	  bestHit.specular = cone.specular;
	  bestHit.smoothness = cone.smoothness;
	  bestHit.emission = cone.emission;
	}*/
} //IntersectTriangularConeMirror

void IntersectHemisphereMirror(Ray ray, inout RayHit bestHit, HemisphereMirror hemisphere)
{
	// Calculate distance along the ray where the sphere is intersected

	// hemisphere.
	//	float4x4 localToWorldMatrix;
	float4x4 frame = hemisphere.localToWorldMatrix; // World is the Unity world
	float3 spherePos = float3(frame[0][3], frame[1][3], frame[2][3]);


	float3 d = ray.origin - spherePos;
	float p1 = -dot(ray.direction, d);
	float p2sqr = p1 * p1 - dot(d, d) + hemisphere.sphereRadius * hemisphere.sphereRadius;
	if (p2sqr < 0)
		return;
	float p2 = sqrt(p2sqr);
	float t = p1 - p2 > 0 ? p1 - p2 : p1 + p2;

	if (t > 0 && t < bestHit.distance)
	{
		bestHit.distance = t;
		bestHit.position = ray.origin + t * ray.direction;
		bestHit.normal = normalize(bestHit.position - spherePos);
		bestHit.albedo = hemisphere.albedo;
		bestHit.specular = hemisphere.specular;
		bestHit.smoothness = hemisphere.smoothness;
		bestHit.emission = hemisphere.emission;
	}
} //IntersectHemisphereMirror

void IntersectSphere(Ray ray, inout RayHit bestHit, Sphere sphere)
{
	// Calculate distance along the ray where the sphere is intersected
	float3 d = ray.origin - sphere.position;
	float p1 = -dot(ray.direction, d);
	float p2sqr = p1 * p1 - dot(d, d) + sphere.radius * sphere.radius;
	if (p2sqr < 0)
		return;
	float p2 = sqrt(p2sqr);
	float t = p1 - p2 > 0 ? p1 - p2 : p1 + p2;

	if (t > 0 && t < bestHit.distance)
	{
		bestHit.distance = t;
		bestHit.position = ray.origin + t * ray.direction;
		bestHit.normal = normalize(bestHit.position - sphere.position);
		bestHit.albedo = sphere.albedo;
		bestHit.specular = sphere.specular;
		bestHit.smoothness = sphere.smoothness;
		bestHit.emission = sphere.emission;
	}
} //IntersectSphere

void IntersectParaboloidMirror(Ray ray, inout RayHit bestHit,
	ParaboloidMirror paraboloid)
{


	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);

	// Calculate distance along the ray where the paraboloid is intersected
	// The computation of intersection is done relative to the  OpenGL local frame of the
	// camera: Z- : look down, X=right, Y: up
	// The equation for the paraboloid: z + d = - (x^2/a^2 + y^2/a^2)
	float3 dir = ray.localDirectionInCamera;
	// dir goes down along the negative z axis of the camera (openGL convention)
	float d = paraboloid.distanceToOrigin; // d is relative to the camera 
	float a = paraboloid.coefficientA;
	float A = ((dir.x * dir.x) + (dir.y * dir.y)) / (a * a);
	float B = dir.z;
	// A * t^2 + B*t + d = 0

	float D = B * B - 4 * A * d;



	//if (D < 0) { // D being neative means that there is no real solution to the equation;
	//  _RayDirectionBuffer[id.y * width + id.x] = float4(ray.direction, 0);
	//  _AccumRayEnergyBuffer[id.y * width + id.x] = float4(dir, bestHit.distance);
	//  _SpecularBuffer[id.y * width + id.x] = float4(A, B, D, d);
	//  return; // no hit; bestHit.distance will remain to be 1.#INF
	//}

	float t = (-B - sqrt(D)) / (2 * A); // -B > 0; choose the lesser t

	/*_RayDirectionBuffer[id.y * width + id.x] = float4(ray.direction, 0);
	_AccumRayEnergyBuffer[id.y * width + id.x] = float4(dir, t);
	_SpecularBuffer[id.y * width + id.x] = float4(A, B, D, d);*/

	if (t > 0 && t < bestHit.distance)
	{
		bestHit.distance = t;

		float3 posInCamera = dir * t; // the intersection point on the paraboloid in the OpenGL Camera space
		bestHit.position = mul(_CameraToWorld, float4(posInCamera, 1.0f)).xyz;

		// gradP = (2x/a^2, 2y/a^2, 1)
		float3 gradP = float3(2 * posInCamera.x / (a * a), 2 * posInCamera.y / (a * a), 1);
		float3 normGradP = length(gradP);
		float3 unitNormalInCamera = gradP / normGradP;
		float3 unitNormal = mul(_CameraToWorld, float4(unitNormalInCamera, 0.0f)).xyz;


		bestHit.normal = unitNormal;
		bestHit.albedo = paraboloid.albedo;
		bestHit.specular = paraboloid.specular;
		bestHit.smoothness = paraboloid.smoothness;
		bestHit.emission = paraboloid.emission;
	}
} //IntersectParaboloidMirror

// https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-rendering-a-triangle/barycentric-coordinates#:~:text=Barycentric%20coordinates%20are%20also%20known,A%2C%20B%2C%20C).

bool IntersectTriangle_MT97(Ray ray, float3 vert0, float3 vert1, float3 vert2,
	out float t, out float u, out float v)
{  // vert0, vert1, and vert2 are coordinates in the Unity world space
   // The origin of the coordinate system is A: P = A + u*AB + v * AC
	// = A + u(B-A) + v(C-A) = A + uB -uA + vC - vA = (1-u-v) A + u B + v C
	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);

	t = 1.#INF;
	// find vectors for two edges sharing vert0
	float3 edge1 = vert1 - vert0;
	float3 edge2 = vert2 - vert0;

	// begin calculating determinant - also used to calculate U parameter
	float3 pvec = cross(ray.direction, edge2); // ray.direction is the direction in the Unity world space

	// if determinant is near zero, ray lies in plane of triangle
	float det = dot(edge1, pvec);

	//// use backface culling
	//if (det < EPSILON) {
	 //// _IntersectionBuffer[id.y * width + id.x] = float4(1.#INF, 1.#INF, 1.#INF, 0);

	 // return false;
	//}

	// the double sided triangle
	if (abs(det) < EPSILON)
		return false;

	float inv_det = 1.0f / det;

	// calculate distance from vert0 toray.origin
	float3 tvec = ray.origin - vert0;

	// calculate U parameter and test bounds
	u = dot(tvec, pvec) * inv_det;
	if (u < 0.0 || u > 1.0f)
	{
		v = 1.#INF;
		// _IntersectionBuffer[id.y * width + id.x] = float4(u,v, 1.#INF, 0);

		return false;
	}

	// prepare to test V parameter
	float3 qvec = cross(tvec, edge1);

	// calculate V parameter and test bounds
	v = dot(ray.direction, qvec) * inv_det;
	if (v < 0.0 || u + v > 1.0f)
	{
		// _IntersectionBuffer[id.y * width + id.x] = float4(u, v, 1.#INF, 0);
		return false;
	}
	// calculate t, ray intersects triangle
	t = dot(edge2, qvec) * inv_det;

	// _IntersectionBuffer[id.y * width + id.x] = float4(u, v, t, 0);

	return true;
} //IntersectTriangle_MT97

void IntersectTriangularConeMirror(Ray ray, inout RayHit bestHit,
	TriangularConeMirror meshObj)
{


	uint offset = meshObj.indices_offset;

	uint count = offset + meshObj.indices_count;

	for (uint i = offset; i < count; i += 3)
	{

		// get the current triangle defined by v0, v1, and v2
		float3 v0 = (mul(meshObj.localToWorldMatrix,
			float4(_Vertices[_Indices[i]], 1))).xyz;
		float3 v1 = (mul(meshObj.localToWorldMatrix,
			float4(_Vertices[_Indices[i + 1]], 1))).xyz;
		float3 v2 = (mul(meshObj.localToWorldMatrix,
			float4(_Vertices[_Indices[i + 2]], 1))).xyz;


		/*StructuredBuffer<float3> _Vertices;
		StructuredBuffer<int> _Indices;
		StructuredBuffer<float2> _UVs;*/

		//_VertexBufferRW[_Indices[i]] = v0;
		//_VertexBufferRW[_Indices[i + 1]] = v1;
		//_VertexBufferRW[_Indices[i + 2]] = v2;

		// changed by Moon Jung, 2020/1/21
		// get the uv coords of the three vertices of the current triangle

		//float3x2 vtxUVs = float3x2(_UVs[_Indices[i]], _UVs[_Indices[i + 1]], _UVs[_Indices[i + 2]]);
		float t, u, v;


		if (IntersectTriangle_MT97(ray, v0, v1, v2, t, u, v))
		{


			// find the nearest hit point
			if (t > 0 && t < bestHit.distance)
			{


				bestHit.distance = t;
				bestHit.position = ray.origin + t * ray.direction;
				bestHit.uvInTriangle = float2(u, v);

				// added by Moon Jung, 2020/1/21
				//bestHit.vtxUVs = vtxUVs;

				bestHit.normal = normalize(cross(v1 - v0, v2 - v0));


				//changed by Moon Jung, 2020/1/20
				bestHit.albedo = meshObj.albedo;
				bestHit.specular = meshObj.specular;
				bestHit.smoothness = meshObj.smoothness;
				bestHit.emission = meshObj.emission;

			} // a nearer point intersected
		} // intersected
	} // for all triangles of the mesh

} // IntersectTriangularConeMirror

void IntersectMeshObject(Ray ray, inout RayHit bestHit, MeshObject meshObj)
{

	// for debugging

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);



	uint offset = meshObj.indices_offset; // the starting index of the mesh within _Indices list

	uint count = offset + meshObj.indices_count; // the count of the indices of the mesh

	for (uint i = offset; i < count; i += 3)
	{

		// get the current triangle defined by v0, v1, and v2
		float3 v0 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i]], 1))).xyz;
		float3 v1 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i + 1]], 1))).xyz;
		float3 v2 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i + 2]], 1))).xyz;


		/*StructuredBuffer<float3> _Vertices;
		StructuredBuffer<int> _Indices;
		StructuredBuffer<float2> _UVs;*/

		//_VertexBufferRW[_Indices[i]] = v0;
		//_VertexBufferRW[_Indices[i+1]] = v1;
		//_VertexBufferRW[_Indices[i+2]] = v2;

		// changed by Moon Jung, 2020/1/21
		// get the uv coords of the three vertices of the current triangle

		float3x2 vtxUVs = float3x2(_UVs[_Indices[i]], _UVs[_Indices[i + 1]], _UVs[_Indices[i + 2]]);
		float t, u, v;


		if (IntersectTriangle_MT97(ray, v0, v1, v2, t, u, v))
		{


			// find the nearest hit point
			if (t > 0 && t < bestHit.distance)
			{
				bestHit.distance = t;
				bestHit.position = ray.origin + t * ray.direction;
				bestHit.uvInTriangle = float2(u, v);

				// added by Moon Jung, 2020/1/21
				bestHit.vtxUVs = vtxUVs;

				bestHit.normal = normalize(cross(v1 - v0, v2 - v0));

				//changed by Moon Jung, 2020/1/20
				bestHit.albedo = meshObj.albedo;
				bestHit.specular = meshObj.specular;
				bestHit.smoothness = meshObj.smoothness;
				bestHit.emission = meshObj.emission;



			} // a nearer point intersected
		} // intersected
	} // for all triangles of the mesh


} //IntersectMeshObject

void IntersectPanoramaMeshObject(Ray ray, inout RayHit bestHit, PanoramaMesh meshObj)
{


	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);

	uint offset = meshObj.indices_offset;

	uint count = offset + meshObj.indices_count;

	for (uint i = offset; i < count; i += 3)
	{

		// get the current triangle defined by v0, v1, and v2
		float3 v0 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i]], 1))).xyz;
		float3 v1 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i + 1]], 1))).xyz;
		float3 v2 = (mul(meshObj.localToWorldMatrix, float4(_Vertices[_Indices[i + 2]], 1))).xyz;


		/*StructuredBuffer<float3> _Vertices;
		StructuredBuffer<int> _Indices;
		StructuredBuffer<float2> _UVs;*/

		/*_VertexBufferRW[_Indices[i]] = v0;
		_VertexBufferRW[_Indices[i + 1]] = v1;
		_VertexBufferRW[_Indices[i + 2]] = v2;*/

		// changed by Moon Jung, 2020/1/21
		// get the uv coords of the three vertices of the current triangle

		float3x2 vtxUVs = float3x2(_UVs[_Indices[i]], _UVs[_Indices[i + 1]], _UVs[_Indices[i + 2]]);
		float t, u, v;


		if (IntersectTriangle_MT97(ray, v0, v1, v2, t, u, v))
		{


			// find the nearest hit point
			if (t > 0 && t < bestHit.distance)
			{


				bestHit.distance = t;
				bestHit.position = ray.origin + t * ray.direction;
				bestHit.uvInTriangle = float2(u, v);

				// added by Moon Jung, 2020/1/21
				bestHit.vtxUVs = vtxUVs;

				bestHit.normal = normalize(cross(v1 - v0, v2 - v0));

				//changed by Moon Jung, 2020/1/20
				bestHit.albedo = meshObj.albedo;
				bestHit.specular = meshObj.specular;
				bestHit.smoothness = meshObj.smoothness;
				bestHit.emission = meshObj.emission;



			} // a nearer point intersected
		} // intersected
	} // for all triangles of the mesh


} //IntersectPanoramaMeshObject


//-------------------------------------
//- SAMPLING

float3x3 GetTangentSpace(float3 normal)
{
	// Choose a helper vector for the cross product
	float3 helper = float3(1, 0, 0);
	if (abs(normal.x) > 0.99f)
		helper = float3(0, 0, 1);

	// Generate vectors
	float3 tangent = normalize(cross(normal, helper));
	float3 binormal = normalize(cross(normal, tangent));
	return float3x3(tangent, binormal, normal);
}

float3 SampleHemisphere(float3 normal, float alpha)
{
	// Sample the hemisphere, where alpha determines the kind of the sampling
	float cosTheta = pow(rand(), 1.0f / (alpha + 1.0f));
	float sinTheta = sqrt(1.0f - cosTheta * cosTheta);
	float phi = 2 * PI * rand();
	float3 tangentSpaceDir = float3(cos(phi) * sinTheta, sin(phi) * sinTheta, cosTheta);

	// Transform direction to world space
	return mul(tangentSpaceDir, GetTangentSpace(normal));
}

//-------------------------------------
//- SHADE

float SmoothnessToPhongAlpha(float s)
{
	return pow(1000.0f, s * s);
}

//-------------------------------------
//- TRACE the ray by finding the closest hit object and accumulating
// the ray's energy and returning the emission color of the hit surface

RayHit HitThruTriConeMirrorForCreateImage(Ray ray, int bounce)
{

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);


	RayHit bestHit = CreateRayHit(); // initialized to bestHit.distance = 1.#INF

	uint count = 0, stride = 0, i = 0;

	//// Trace ground plane
	//IntersectGroundPlane(ray, bestHit);

	//// Trace spheres
	//_Spheres.GetDimensions(count, stride);

	//for (i = 0; i < count; i++)
	//{
	//	IntersectSphere(ray, bestHit, _Spheres[i]);
	//}

	//Added by Moon Jung
	//IntersectConeMirror(ray, bestHit, _ConeMirrors[0]);

	// if the bounce = 0, the ray hits the mirror; This event
	// treated differently than the ordinary mesh objects

	TriangularConeMirror meshObj = _TriangularConeMirrors[0];

	if (bounce == 0)
	{

		// consider only  the intersection of the ray with the mirror object
		// when the ray hits the object for the first time

		IntersectTriangularConeMirror(ray, bestHit, meshObj);

		//1.#INF
		//if (bestHit.distance < 1.#INF)
		//{

		  // check if the ray has hit the forbidden region of the mirror;
		  // In that case, the hit point is considered at INFINITY so that 
		  // the color of that point becomes black.
		  // This is relevant only when the predistorted image is created
		  // 

		//	float3 rayVector = ray.direction * bestHit.distance;


		//	float rayDistAlongCameraViewDir = dot(_CameraViewDirection, rayVector);
		  // check if the rayVector lies in the forbidden region
		//	float penetrationDistIntoMirror = rayDistAlongCameraViewDir - meshObj.distanceToOrigin;

		  // debug
		  //_AccumRayEnergyBuffer[id.y * width + id.x]  = float4(rayVector, _CaptureOrProjectOrObserve);

		  //_EmissionBuffer[id.y * width + id.x] = float4(rayDistAlongCameraViewDir, meshObj.height,
		  //	meshObj.notUseRatio, penetrationDistIntoMirror);

		//	if (penetrationDistIntoMirror <= meshObj.notUseRatio * meshObj.height)
		//	{ // the ray hits the forbidden region of the mirror
		//		bestHit.distance = 1.#INF;

		//	}

		//} //if (bestHit.distance < 1.#INF) 

		return bestHit;
	} //if (bounce == 0) 
	else
	{
		// when the ray has hit the mirror object, that is, when its bounce is greater than 0,
		// consider all the objects for intersection of the ray

		IntersectPanoramaMeshObject(ray, bestHit, _PanoramaMeshes[0]); // hit or not hit

		//_SpecularBuffer[id.y * width + id.x] = float4(ray.direction, i); // the 


		/*_MeshObjects.GetDimensions(count, stride);

		for (i = 0; i < count; i++) {
		  IntersectMeshObject(ray, bestHit, _MeshObjects[i]);
		}
	*/
	//_IntersectionBuffer[id.y * width + id.x] = float4(bestHit.position, bestHit.distance);
		return bestHit;
	}
} // HitThruTriConeMirrorForCreateImage

RayHit HitThruParaboloidMirrorForCreateImage(Ray ray, int bounce)
{

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);


	RayHit bestHit = CreateRayHit(); // initialized to bestHit.distance = 1.#INF

	uint count = 0, stride = 0, i = 0;

	//// Trace ground plane
	//IntersectGroundPlane(ray, bestHit);

	//// Trace spheres
	//_Spheres.GetDimensions(count, stride);

	//for (i = 0; i < count; i++)
	//{
	//	IntersectSphere(ray, bestHit, _Spheres[i]);
	//}

	//Added by Moon Jung
	//IntersectConeMirror(ray, bestHit, _ConeMirrors[0]);

	// if the bounce = 0, the ray hits the mirror; This event
	// treated differently than the ordinary mesh objects

	ParaboloidMirror paraboloid = _ParaboloidMirrors[0];
	if (bounce == 0)
	{

		// consider only  the intersection of the ray with the mirror object
		// when the ray hits the object for the first time

		IntersectParaboloidMirror(ray, bestHit, paraboloid);

		//if (bestHit.distance < 1.#INF)
		//{

		  // check if the ray has hit the forbidden region of the mirror

		//	float3 rayVector = ray.direction * bestHit.distance;


		//	float rayDistAlongCameraViewDir = dot(_CameraViewDirection, rayVector);
		  // check if the rayVector lies in the forbidden region
		//	float penetrationDistIntoMirror = rayDistAlongCameraViewDir - paraboloid.distanceToOrigin;

		  // debug
		  //_AccumRayEnergyBuffer[id.y * width + id.x] = float4(rayVector, _CaptureOrProjectOrObserve);

		  //_EmissionBuffer[id.y * width + id.x] = float4(rayDistAlongCameraViewDir, 
		  //	paraboloid.height, paraboloid.notUseRatio, penetrationDistIntoMirror);

		//	if (penetrationDistIntoMirror <= paraboloid.notUseRatio * paraboloid.height)
		//	{ // the ray hits the forbidden region of the mirror
		//		bestHit.distance = 1.#INF;

		//	}

		//} //if (bestHit.distance < 1.#INF) 


		return bestHit;
	} // if (bounce == 0)
	else
	{
		// when the ray has hit the mirror object, that is, when its bounce is greater than 0,
		// consider all the objects for intersection of the ray

		IntersectPanoramaMeshObject(ray, bestHit, _PanoramaMeshes[0]); // hit or not hit

		//_SpecularBuffer[id.y * width + id.x] = float4(ray.direction, i); // the 


		/*_MeshObjects.GetDimensions(count, stride);

		for (i = 0; i < count; i++) {
		  IntersectMeshObject(ray, bestHit, _MeshObjects[i]);
		}
	*/
	//_IntersectionBuffer[id.y * width + id.x] = float4(bestHit.position, bestHit.distance);
		return bestHit;
	}

} // HitThruParaboloidMirrorForCreateImage


RayHit HitThruHemisphereMirrorForCreateImage(uint3 id, Ray ray, int bounce)
{
	// This function is called for bounce 0, that is the ray that started from the camera,
	//  bounce 1, that is the ray that  was reflected by the mirror object.

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);


	RayHit bestHit = CreateRayHit(); // initialized to bestHit.distance = 1.#INF

	uint count = 0, stride = 0, i = 0;


	if (bounce == 0)
	{
		// This is when the ray from the camera  hits an object for the first time

		HemisphereMirror hemisphere = _HemisphereMirrors[0];

		IntersectHemisphereMirror(ray, bestHit, hemisphere);

		if (bestHit.distance < 1.#INF)
		{

			//check if the ray has hit the not-used bottom region of the mirror

			float3 rayVector = ray.direction * bestHit.distance; // rayVector is a global vector in Unity WorldSpace

			/*float rayDistAlongCameraViewDir = dot(rayVector,
				                                float3( _CameraViewDirectionInUnitySpace.x,
													    _CameraViewDirectionInUnitySpace.y,
								                         _CameraViewDirectionInUnitySpace.z )
								                );*/

			//float rayDistAlongCameraViewDir = dot(rayVector, _CameraViewDirection);
			float rayDistAlongCameraViewDir = dot(rayVector, _CameraForwardDirection);
			//check if the rayVector lies in the forbidden region
			float penetrationDistIntoMirror = rayDistAlongCameraViewDir
				                                 - hemisphere.distanceToOrigin;

			//debug


			//_EmissionBuffer[id.y * width + id.x] = float4(rayVector, penetrationDistIntoMirror);
		  // _AccumRayEnergyBuffer[id.y * width + id.x] = float4(_CameraViewDirection,
		 //	                                              rayDistAlongCameraViewDir );
			//_AccumRayEnergyBuffer[id.y * width + id.x] = float4(hemisphere.notUsedHeightRatio,
			//	                                                hemisphere.usedHeight,
			//	                                                penetrationDistIntoMirror,
			//		                                            penetrationDistIntoMirror/ hemisphere.usedHeight);

		 
		   
			// paraboloid.height, paraboloid.notUseRatio, penetrationDistIntoMirror);
			 //compute the effectiveHeight used for reflection
			 //float effectiveHeight = (1 - hemisphere.notUsedHeightRatio) * hemisphere.height;
			if (penetrationDistIntoMirror >= hemisphere.usedHeight)
			{ // the ray hits the forbidden region of the mirror
				bestHit.distance = 1.#INF;

			}

		} //if (bestHit.distance < 1.#INF) 

		else
		{
			//_EmissionBuffer[id.y * width + id.x] = float4(ray.direction, 0);
			//_AccumRayEnergyBuffer[id.y * width + id.x] = float4(_CameraForwardDirection.x,
			//	                                                _CameraForwardDirection.y,
			//	                                                _CameraForwardDirection.z, 0);
			
		}

		return bestHit;
	} // if (bounce == 0)
	else // bounce ==1
	{
		// This is when the ray has hit the panorama screen, that is, when its bounce is greater than 0.
		
		IntersectPanoramaMeshObject(ray, bestHit, _PanoramaMeshes[0]); // hit or not hit

		//_SpecularBuffer[id.y * width + id.x] = float4(ray.direction, i); // the 

		/*_MeshObjects.GetDimensions(count, stride);

		for (i = 0; i < count; i++) {
		  IntersectMeshObject(ray, bestHit, _MeshObjects[i]);
		}
	*/
	//_IntersectionBuffer[id.y * width + id.x] = float4(bestHit.position, bestHit.distance);
		return bestHit;
	}

} // HitThruHemisphereMirrorForCreateImage

//-------------------------------------
//- TRACE the ray by finding the closest hit object and accumulating
// the ray's energy and returning the emission color of the hit surface

// Shade(ray,hit), which is the emission color of the hit
// hit point, is multiplied to the accumulated ray.energy
// which accounts the attenuation of the emission color due
// to the light transmission from the camera to the hit point.
// If the light transmission path is long, then the accumulated ray
// energy is weak and so the emission color of the hit point contributes
// the rendered image only partially.
// If the attenudated ray energy was zero, the ray is NOT traced any more.
float3 ShadeForCreateImage(uint3 id, inout Ray ray, RayHit hit)
{   // This function is called for each bounce of the ray.

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);


	// This function, originally Shade() function, is called  when the mirror or the panorama screen  is hit.
	// Compute the reflection ray at the hit point for the new bounce of ray.
	// Get the emission color of the hit point, and the current attenudated energy of the ray which
	// is computed by means of the specular reflection at the point.
	// Return the ray state together with the emission color of the ray at the hit point.

	// This is the simple version where diffuse effect is not considered.
	// For this, use the following paragraph commented out

	ray.origin = hit.position + hit.normal * 0.001;

	ray.direction = reflect(ray.direction, hit.normal);

	ray.energy *= hit.specular; // hit.specular is (1.0, 1.0, 1.0) by default for the mirror mesh
	                            // and is (0.1, 0.1, 0.1) by default for the panorama screen

	
	//The fuller version: Consider the diffuse shading( soft shadow, ambient occlusion,
	 //diffuse global illumination):
   // Calculate chances of diffuse and specular reflection

  //hit.albedo = min(1.0f - hit.specular, hit.albedo);
  //float specChance = energy(hit.specular);
  //float diffChance = energy(hit.albedo);

  //// Roulette-select the ray's path
  //float roulette = rand();
  //if (roulette < specChance) {
  //	// Specular reflection
  //	ray.curPixelin = hit.position + hit.normal * 0.001f;
  //	float alpha = SmoothnessToPhongAlpha(hit.smoothness);

  //	ray.direction = SampleHemisphere(
  //		reflect(ray.direction, hit.normal), alpha);

  //	float f = (alpha + 2) / (alpha + 1);
  //	ray.energy *= (1.0f / specChance) * hit.specular * sdot(hit.normal, ray.direction, f);

  //}
  //else if (diffChance > 0 && roulette < specChance + diffChance) {
  //	// Diffuse reflection
  //	ray.curPixelin = hit.position + hit.normal * 0.001f;
  //	ray.direction = SampleHemisphere(hit.normal, 1.0f);
  //	ray.energy *= (1.0f / diffChance) * hit.albedo;
  //}
  //else {
  //	// Terminate ray: The accumulated ray energy is zero
  //	ray.energy = 0.0f;
  //}

  // return the emission color of the hit point
  // check if the hit point emits color from the panorama screen to which 
	// texture image is mapped. This is indicated by emission =-1, which
  // is returned by Trace(ray):

	if (hit.emission.x < 0 && hit.emission.y < 0 && hit.emission.z < 0)
	{ // hit the panorama screen. Get the texture color at the hit point.

		// compute the emission color from the texture mapping
		// <see href="https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-samplelevel">HERE</href>
		  // SampleLevel -> is similar to Sample except that is uses the LOD level (in the last component of the location parameter) 
		  // to choose the mipmap level. For example, a 2D texture uses the first two components for uv coordinates and the third
		  // component for the mipmap level.        

	
		float2 uv = hit.uvInTriangle; // get the barycentric coord of the hit point

		 // Conversion from the barycentric coordinates to the cartesian cooridnates - Added by Moon
		// // The origin of the coordinate system is A: P = A + u*AB + v * AC
	     // = A + u(B-A) + v(C-A) = A + uB -uA + vC - vA = (1-u-v) A + u B + v C
		float2 uvTex = (1 - uv[0] - uv[1]) * hit.vtxUVs[0]
			                       + uv[0] * hit.vtxUVs[1]
			                       + uv[1] * hit.vtxUVs[2];
		float3 emission;
		// uvTex is the texture coordinate of the point on the panorama screen hit by the
		// current ray.


		if (_NumOfTargetTextures == 1)
		{
				emission = _RoomTexture0.SampleLevel(sampler_RoomTexture0,
					                                float2(uvTex.x, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

		}
		else if (_NumOfTargetTextures == 2)
		{
			if (uvTex.x <= 1 / 2.0)
			{
				emission = _RoomTexture0.SampleLevel(sampler_RoomTexture0,
					float2(uvTex.x * 2, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else  // (uvTex.x <= 1)
			{
				emission = _RoomTexture1.SampleLevel(sampler_RoomTexture1,
					float2(uvTex.x * 2 - 1.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}

		}
		else if  (_NumOfTargetTextures == 3)
		{

			if (uvTex.x <= 1 / 3.0)
			{
				emission = _RoomTexture0.SampleLevel(sampler_RoomTexture0,
					float2(uvTex.x * 3, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else if (uvTex.x <= 2 / 3.0)
			{
				emission = _RoomTexture1.SampleLevel(sampler_RoomTexture1,
					float2(uvTex.x * 3 - 1.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else // (uvTex.x <= 3 / 3.0)
			{
				emission = _RoomTexture2.SampleLevel(sampler_RoomTexture2,
					float2(uvTex.x * 3 - 2.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
		}

		else //  (_NumOfTargetTextures == 4)
		{
			if (uvTex.x <= 1 / 4.0)
			{
				emission = _RoomTexture0.SampleLevel(sampler_RoomTexture0,
					float2(uvTex.x * 4, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else if (uvTex.x <= 2 / 4.0)
			{
				emission = _RoomTexture1.SampleLevel(sampler_RoomTexture1,
					float2(uvTex.x * 4 - 1.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else if (uvTex.x <= 3 / 4.0)
			{
				emission = _RoomTexture2.SampleLevel(sampler_RoomTexture2,
					float2(uvTex.x * 4 - 2.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
			else
			{
				emission = _RoomTexture3.SampleLevel(sampler_RoomTexture3,
					float2(uvTex.x * 4 - 3.0, uvTex.y), 0).xyz;
				//emission = float3(0, 0, 0);

			}
		} // //  (_NumOfTargetTextures == 4)
		
		//debug
		/*uint uvx = (uint)(uvTex.x * width);
		uint uvy = (uint)(uvTex.y * height);

		_Result[uint2(uvx, uvy)] = float4(1, 0, 0, 1);*/
		// hit 1 in float4(uvTex.x, uvTex.y, 0, 1) means that the current pixel is hit
		//_AccumRayEnergyBuffer[id.y * width + id.x] = float4(uvTex.x, uvTex.y, 0, 1);
		//_EmissionBuffer[id.y * width + id.x] = float4( emission.x, emission.y,emission.z, 1);

		return emission;
	}
	else
	{  // // hit the mirror object.  Get the emission color of the mirror hit point, 
		// which is the black color (no emission color) by default.
		// _EmissionBuffer[id.y * width + id.x] = hit.emission;

		//_AccumRayEnergyBuffer[id.y * width + id.x] = float4(0,0, 0, 0);
		return hit.emission; 
		
	}


} // ShadeForCreateImage




float2 get_undistorted_ndc_newton(float2 p_d, in CameraParams cameraParams, float2 thresholdNewton /* = 0.01 */)
{
	/*int i = 0;
	while (i < N) {
	  i += 1;
	  d = 1 + k1 * (s * s + t * t) + k2 * (s * s * s * s) + (2 * s * s * t * t) + (t * t * t * t);

	  f1 = -u + (s * d + (2 * p1 * s * t + p2 * (s * s + t * t + 2 * s * s))) * fx * cx;
	  f2 = -v + (t * d + (p1 * (s * s + t * t + 2 * t * t) + 2 * p2 * s * t)) * fy + cy;
	  j1s = fx * (1 + k1 * (3 * s * s + t * t) + k2 * ((5 * s * s + 6 * t * t) * s * s + t * t * t * t)) + 2 * p1 * fx * t + 6 * p2 * fx * s;
	  j1t = fx * (2 * k1 * s * t + 4 * k2 * (s * s * s * t + s * t * t * t)) + 2 * p1 * fx * s + 2 * p2 * fx * t;
	  j2s = fy * (2 * k1 * s * t + 4 * k2 * (s * s * s * t + s * t * t * t)) + 2 * p1 * fy * s + 2 * p2 * fy * t;
	  j2t = fy * (1 + k1 * (s * s + 3 * t * t) + 3 * t * t) + k2 * (s * s * s * s + (6 * s * s + 5 * t * t) * t * t)) + 6 * p1 * fy * t + 2 * p2 * fy * s;

	  d = (j1s * j2t - j1t * j2s);

	  S = s - (j2t * f1 - j1t * f2) / d;
	  T = t - (j2s * f1 - j1s * f2) / d;

	  if (abs(S - s) < err_threshold && abs(T - t) < err_threshold) {
		break;
	  }

	  s = S;
	  t = T;
	}*/
	return float2(0, 0);
}

float2 get_undistorted_ndc_iterative(float2 p_d, in CameraParams cameraParams, float2 thresholdIterative /* = 0.01*/)
{
	float2 p_nuInitialGuess = normalize(p_d.x, p_d.y, cameraParams);
	float2 p_nu = p_nuInitialGuess;

	while (true)
	{
		float2 err = distort_normalized(p_nu.x, p_nu.y, cameraParams);
		err -= p_nuInitialGuess;
		p_nu = p_nu - err;

		++_CurrentCounter;
		if (_CurrentCounter >= _SafeCounter)
		{
			_CurrentCounter = 0;
			break;
		}

		if (err.x < thresholdIterative.x && err.y < thresholdIterative.y)
			break;
	}

	//for (;;) {
	//  float2 t1 = distort_normalized(p_nu.x, p_nu.y, cameraParams);
	//  float2 err = t1 – p_nuInitialGuess;
	//  p_nu -= err;

	//  if (err.x < thresholdIterative.x && err.y < thresholdIterative.y) { break; }
	//}

	float2 p_nu_denormalized = denormalize(p_nu.x, p_nu.y, cameraParams);
	return p_nu_denormalized;
}

float2 get_undistorted_ndc_direct(float2 p_d, in CameraParams cameraParams)
{
	// p_d = the original distorted image coord
	float u = p_d.x;
	float v = p_d.y;

	float s = (u - cameraParams.PrincipalPoint.x) / cameraParams.FocalLength.x;
	float t = (v - cameraParams.PrincipalPoint.y) / cameraParams.FocalLength.y;

	/* x skew_c * y_nu (Skew isn't used since the projector in these days is accurately manufactured.) */; // x_d = normalized distorted x pixel coord;


	// distance = r^2
	float r2 = s *s + t * t;
	// r^4
	float r4 = r2 * r2;

	float k1 = cameraParams.RadialCoefficient.x;
	float k2 = cameraParams.RadialCoefficient.y;
	float p1 = cameraParams.TangentialCoefficient.x;
	float p2 = cameraParams.TangentialCoefficient.y;

	float d1 = k1 * r2 + k2 * r4;
	float d2 = 1 / ( 4 * k1 * r2 + 6 * k2 * r4 + 8 * p1 * t + 8 * p2 * s + 1);

	float xn_u = s - d2 * ( d1 * s + 2 * p1 * s * t + p2 * (r2 + 2 * s * s) );
	float yn_u = t - d2 * ( d1 * t +  p1 * (r2 + 2 * t * t) + 2 * p2 * s * t );
	float x_u = xn_u * cameraParams.FocalLength.x + cameraParams.PrincipalPoint.x;
	float y_u = yn_u * cameraParams.FocalLength.y + cameraParams.PrincipalPoint.y;

	return float2(x_u, y_u);
	//return float2(0, 0);
}


//float2 get_undistorted_ndc_direct(float2 p_d, in CameraParams cameraParams)
//{
//	//y_nu = (y - cy) / fy;
//	//x_nu = (x - cx) / fx – skew_c * y_nu;
//
//	//ru2 = x_nu * x_nu + y_nu * y_nu;	// ru2 = ru*ru
//	//radial_d = 1 + k1 * ru2 + k2 * ru2 * ru2 + k3 * ru2 * ru2 * ru2;
//
//	//x_nd = radial_d * x_nu + 2 * p1 * x_nu * y_nu + p2 * (ru2 + 2 * x_nu * x_nu);
//	//y_nd = radial_d * y_nu + p1 * (ru2 + 2 * y_nu * y_nu) + 2 * p2 * x_nu * y_nu;
//
//	//x_pd = fx * (x_nd + skew_c * y_nd) + cx;
//	//y_pd = fy * y_nd + cy;
//
//	//Iu(x, y) = Id(x_pd, y_pd);
//
//	/////////////////////////////////////
//	/*float y_nu = (p_d.y - cameraParams.PrincipalPoint.y) / cameraParams.FocalLength.y;
//	float x_nu = (p_d.x - cameraParams.PrincipalPoint.x) / cameraParams.FocalLength.x;*/
//
//	//	p_d.x = xn_d * fx + cx;	 
//	//	p_d.y = yn_d * fy + cy;		
//	//
//	//  [p_d.x,  =  [fx 0, * [xn_d, + [cx,
//	//   p_d.y]		 fy 0]	  yn_d]	   cy]
//
//	float yn_d = (p_d.y - cameraParams.PrincipalPoint.y) / cameraParams.FocalLength.y;
//	float xn_d = (p_d.x - cameraParams.PrincipalPoint.x) / cameraParams.FocalLength.x;
//	/* x skew_c * y_nu (Skew isn't used since the projector in these days is accurately manufactured.) */; // x_d = normalized distorted x pixel coord;
//
//
//	// distance = r^2
//	float rsqr = xn_d * xn_d + yn_d * yn_d;
//	// r^4
//	float rqd = rsqr * rsqr;
//
//	float k1 = cameraParams.RadialCoefficient.x;
//	float k2 = cameraParams.RadialCoefficient.y;
//	float p1 = cameraParams.TangentialCoefficient.x;
//	float p2 = cameraParams.TangentialCoefficient.y;
//
//	float d1 = k1 * rsqr + k2 * rqd;
//	float d2 = 1 / ((4 * k1 * rsqr) + (6 * k2 * rqd) + (8 * p1 * yn_d) + (8 * p2 * xn_d + 1));
//
//	float xn_u = xn_d - d2 * (d1 * xn_d + 2 * p1 * xn_d * yn_d + p2 * (rsqr + 2 * xn_d * xn_d));
//	float yn_u = yn_d - d2 * (d1 * yn_d + p1 * (rsqr + 2 * yn_d * yn_d) + 2 * p2 * xn_d * yn_d);
//	float x_u = xn_u * cameraParams.FocalLength.x + cameraParams.PrincipalPoint.x;
//	float y_u = yn_u * cameraParams.FocalLength.y + cameraParams.PrincipalPoint.y;
//
//	return float2(x_u, y_u);
//	//return float2(0, 0);
//}

// const float skew_c = cameraParams.SkewCoefficient;
float2 normalize(float x, float y, in CameraParams cameraParams)
{
	float fx = cameraParams.FocalLength.x;
	float fy = cameraParams.FocalLength.y;

	float cx = cameraParams.PrincipalPoint.x;
	float cy = cameraParams.PrincipalPoint.y;

	float y_n = (y - cy) / fy;
	float x_n = (x - cx) / fx;
	return float2(x_n, y_n);
}

float2 denormalize(float x, float y, in CameraParams cameraParams)
{
	float fx = cameraParams.FocalLength.x;
	float fy = cameraParams.FocalLength.y;

	float cx = cameraParams.PrincipalPoint.x;
	float cy = cameraParams.PrincipalPoint.y;

	float x_p = fx * x + cx;
	float y_p = fy * y + cy;
	return float2(x_p, y_p);
}


float2 distort_normalized(float x_nu, float y_nu, in CameraParams cameraParams)
{
	float k1 = cameraParams.RadialCoefficient.x;
	float k2 = cameraParams.RadialCoefficient.y;
	// k3 is removed.
	//const float k3 = cameraParams.RadialCoefficient.z;

	float p1 = cameraParams.TangentialCoefficient.x;
	float p2 = cameraParams.TangentialCoefficient.y;

	float r2 = x_nu * x_nu + y_nu * y_nu;

	float radial_d = 1.0 + (k1 * r2) + (k2 * r2 * r2); // + (k3 * r2 * r2 * r2);
	float x_nd = (radial_d * x_nu) + (2 * p1 * x_nu * y_nu) + (p2 * (r2 + 2 * x_nu * x_nu));
	float y_nd = (radial_d * y_nu) + (p1 * (r2 + 2 * y_nu * y_nu)) + (2 * p2 * x_nu * y_nu);
	return float2(x_nd, y_nd);
}

[numthreads(8, 8, 1)]
void CreateImageTriConeMirror(uint3 id : SV_DispatchThreadID)
{
	//_CameraPosInWorld = mul(_CameraToWorld, float4((float3) 0, 1.0)).xyz;
	//_CameraViewDirection = -float3(_CameraToWorld[0][2], _CameraToWorld[1][2], _CameraToWorld[0][2]);
	_Pixel = id.xy;

	uint width = 0, height = 0;
	_Result.GetDimensions(width, height);

	float2 undistorted_pixel_coords = (float2) 0;
	switch (_UndistortMode)
	{

	case -1:  // do not consider the lens distortion
		undistorted_pixel_coords.x = (float)id.x;
		undistorted_pixel_coords.y = (float)id.y;
		break;

	case 0:
		undistorted_pixel_coords = get_undistorted_ndc_iterative((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdIterative.xy);
		break;

	case 1:
		undistorted_pixel_coords = get_undistorted_ndc_direct((float2) id.xy, _CameraLensDistortionParams[0]);
		break;

	case 2:
		undistorted_pixel_coords = get_undistorted_ndc_newton((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdNewton.xy);
		break;
	}

	float2 undistorted_ndc = (float2) 0;
	undistorted_ndc.x = (undistorted_pixel_coords.x + _PixelOffset.x) / (float)width * 2.0 - 1.0;
	undistorted_ndc.y = (undistorted_pixel_coords.y + _PixelOffset.y) / (float)height * 2.0 - 1.0;

	Ray ray = CreateCameraRay(undistorted_ndc);
	RayHit hit = (RayHit)0;
	float3 resultAccumulated = (float3) 0;
	float3 emission = (float3) 0;

	for (int i = 0; i < _MaxBounce; ++i)
	{
		hit = HitThruTriConeMirrorForCreateImage(ray, i);
		if (hit.distance == 1.#INF)
		{
			// debug: not hit
			break;
		}
		else // hit
		{
			resultAccumulated += ray.energy * ShadeForCreateImage(id,ray, hit);

			if (!any(ray.energy))
			{
				break;
			}
		}
		_Result[id.xy] = float4(resultAccumulated, 1);
	}
}//CreateImageTriConeMirror(uint3 id : SV_DispatchThreadID)

[numthreads(8, 8, 1)]
void CreateImageGeoConeMirror(uint3 id : SV_DispatchThreadID)
{
	// id.xy : pd_x 와 pd_y. pixel coords (0 ~ resolution.width && height)
// Get the camera position and the view direction in the world
	//_CameraPosInWorld = mul(_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;
	// Get the Z axis of the Camera
	_CameraViewDirection = -float3(_CameraToWorld[0][2], _CameraToWorld[1][2], _CameraToWorld[0][2]);

	//_Pixel = id.xy;

	  // id.xy == x_pd, y_pd.
	// undistorted_pixel_coords == x_pu, y_pu.
	float2 undistorted_pixel_coords = (float2) 0;
	switch (_UndistortMode)
	{

	case -1:  // do not consider the lens distortion
		undistorted_pixel_coords.x = (float)id.x;
		undistorted_pixel_coords.y = (float)id.y;
		break;
	case 0:
		undistorted_pixel_coords = get_undistorted_ndc_iterative((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdIterative.xy);
		break;

	case 1:
		undistorted_pixel_coords = get_undistorted_ndc_direct((float2) id.xy, _CameraLensDistortionParams[0]);
		break;

	case 2:
		undistorted_pixel_coords = get_undistorted_ndc_newton((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdNewton.xy);
		break;
	}

	// Get the dimensions of the RenderTexture
	uint width = 0, height = 0;
	_Result.GetDimensions(width, height);

	float2 undistorted_ndc = (float2) 0;
	undistorted_ndc.x = ((float)undistorted_pixel_coords.x + _PixelOffset.x) / (float)width * 2.0f - 1.0f;
	undistorted_ndc.y = ((float)undistorted_pixel_coords.y + _PixelOffset.y) / (float)height * 2.0f - 1.0f;

	Ray ray = CreateCameraRay(undistorted_ndc);

	// Trace and shade the ray
	// result 는 x_pu, y_pu 의 color.
	float3 result = (float3) 0;
	RayHit hit = (RayHit)0;

	for (int i = 0; i < _MaxBounce; i++)
	{
		//hit = HitThruParaboloidMirrorForCreateImage(ray, i, undistorted_pixel_coords); // when the ray hits the panorama screen, hit.emission will be (-1,-1,-1)    
		if (hit.distance == 1.#INF)
		{
			break;
		}
		else
		{
			result += ray.energy * ShadeForCreateImage(id, ray, hit);

			if (!any(ray.energy))
			{
				break;
			}
		}
	}
	// result 는 x_pu, y_pu 의 color.
	// assign this color to (x_pd, y_pd) which is same as id.xy.
	_Result[id.xy] = float4(result, 1.0);
}// CreateImageGeoConeMirror(uint3 id : SV_DispatchThreadID)

[numthreads(8, 8, 1)]
void CreateImageParaboloidMirror(uint3 id : SV_DispatchThreadID)
{
	// Get the camera position and the view direction in the world
	//_CameraPosInWorld = mul(_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;
	// Get the Z axis of the Camera
	_CameraViewDirection = -float3(_CameraToWorld[0][2], _CameraToWorld[1][2], _CameraToWorld[0][2]);

	_Pixel = id.xy;
	float2 undistorted_pixel_coords = (float2) 0;
	switch (_UndistortMode)
	{

	case -1:  // do not consider the lens distortion
		undistorted_pixel_coords.x = (float)id.x;
		undistorted_pixel_coords.y = (float)id.y;
		break;
	case 0:
		undistorted_pixel_coords = get_undistorted_ndc_iterative((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdIterative.xy);
		break;

	case 1:
		undistorted_pixel_coords = get_undistorted_ndc_direct((float2) id.xy, _CameraLensDistortionParams[0]);
		break;

	case 2:
		undistorted_pixel_coords = get_undistorted_ndc_newton((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdNewton.xy);
		break;
	}

	// Get the dimensions of the RenderTexture
	uint width = 0, height = 0;
	_Result.GetDimensions(width, height);

	float2 undistorted_ndc = (float2) 0;
	undistorted_ndc.x = (undistorted_pixel_coords.x + _PixelOffset.x) / (float)width * 2.0f - 1.0f;
	undistorted_ndc.y = (undistorted_pixel_coords.y + _PixelOffset.y) / (float)height * 2.0f - 1.0f;

	Ray ray = CreateCameraRay(undistorted_ndc);

	// Trace and shade the ray
	float3 result = (float3) 0;
	RayHit hit = (RayHit)0;

	for (int i = 0; i < _MaxBounce; i++)
	{
		hit = HitThruParaboloidMirrorForCreateImage(ray, i); // when the ray hits the panorama screen, hit.emission will be (-1,-1,-1)    
		
		if (hit.distance == 1.#INF) // for any bounce i = 0 or 1, the no hit means no color for the current pixel 
		{
			break;
		}
		else
		{
			result += ray.energy * ShadeForCreateImage(id,ray, hit);

			if (!any(ray.energy))
			{
				break;
			}
		}
	}
	_Result[id.xy] = float4(result, 1.0);
}// CreateImageParaboloidMirror(uint3 id : SV_DispatchThreadID)

[numthreads(8, 8, 1)]
//https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/sv-dispatchthreadid#:~:text=Indices%20for%20which%20combined%20thread%20and%20thread%20group,across%20the%20range%20specified%20in%20Dispatch%20and%20numthreads.

//SV_DispatchThreadID is the sum of SV_GroupID* numthreads and GroupThreadID.
//It varies across the range specified in Dispatch and numthreads.
//For example if Dispatch(2, 2, 2) is called on a compute shader with numthreads(3, 3, 3)
//SV_DispatchThreadID will have a range of 0..5 for each dimension.
// id, a SV_DisptachThreadID, ranges over the screen width and height.
void CreateImageHemisphereMirror(uint3 id : SV_DispatchThreadID)
{
	//_CameraPosInWorld = mul(_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;
	// Get the Z axis of the Camera
	_CameraViewDirection = -float3(_CameraToWorld[0][2], _CameraToWorld[1][2], _CameraToWorld[0][2]);
	//https://www.gamasutra.com/blogs/DavidKuri/20180504/317575/GPU_Ray_Tracing_in_Unity__Part_1.php
	//https://www.gamedev.net/tutorials/programming/graphics/ray-tracing-part-1-r3556/
	//https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays

	// (u,v, z) = ( w/h (2*x/w -1), 2*y/h -1, 1/tan(theta/2) )
	_Pixel = id.xy;

	// Get the dimensions of the RenderTexture
	uint width, height;
	_Result.GetDimensions(width, height);

	float2 undistorted_pixel_coords = (float2) 0;



	//Here, the undistorted pixel coordinates are equivalent to pixel coordinates generated
	//  with the pinhole camera model of a standard graphics pipeline when the
	//	camera intrinsic parameters cx, cy, fx, fy are accounted for in the projection matrix

	switch (_UndistortMode)
	{
	case -1:  // do not consider the lens distortion
		undistorted_pixel_coords.x = (float)id.x;
		undistorted_pixel_coords.y = (float)id.y;
		break;

	case 0: // (x,y) are the real image coordinates that are considered to be distorted from undistorted_pixel_coords
		undistorted_pixel_coords = get_undistorted_ndc_iterative((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdIterative.xy);
		break;

	case 1:
		undistorted_pixel_coords = get_undistorted_ndc_direct((float2) id.xy, _CameraLensDistortionParams[0]);
		break;

	case 2:
		undistorted_pixel_coords = get_undistorted_ndc_newton((float2) id.xy, _CameraLensDistortionParams[0], _ThresholdNewton.xy);
		break;
	}

	float2 undistorted_ndc = (float2) 0;
	//// Transform pixel to [-1,1] range; _PixelOffset ranges over (0,1) pixels:
	undistorted_ndc.x = (undistorted_pixel_coords.x + _PixelOffset.x) / (float)width * 2.0f - 1.0f;
	undistorted_ndc.y = (undistorted_pixel_coords.y + _PixelOffset.y) / (float)height * 2.0f - 1.0f;

	Ray ray = CreateCameraRay(undistorted_ndc);

	float3 result = float3(0, 0, 0);
	float3 emission;

	RayHit hit;

	for (int i = 0; i < _MaxBounce; i++)
	{

		hit = HitThruHemisphereMirrorForCreateImage(id, ray, i); // when the ray hits the panorama screen, hit.emission will be (-1,-1,-1)
		
		if (hit.distance == 1.#INF) // for any bounce i = 0 or 1, the no hit means no color for the current pixel
		{
			// the fourth  0 in float4(0,0,0,0) means that the current pixel is hit
			//_AccumRayEnergyBuffer[id.y * width + id.x] = float4(0,0,0,0);

			break; // break with current result =(0,0,0), the black color
		}
		else // hit the mirror or the panorama screen
		{   // ray.energy = float3(1.0f, 1.0f, 1.0f) when the ray is initialized
			result += ray.energy * ShadeForCreateImage(id,ray, hit); 
			//  ShadeForCreateImage(ray, hit,i) returns the emission color at the hit point of the current bounce i 
			// of the ray. It is (0,0,0) in the case of the mirror and the texture color in the case of the panorama screen
			// It also computes the attenuated enery of the ray for the new bounce of ray;
			// It is computed by means of the specular reflection at the point.
			
			if (!any(ray.energy)) 
				break;  // break with the current result color when the attenuation energy of the new ray reaches zero
		}
	}
	//In Image _Result[], the screenspace origin is at the bottom left.
	// Only the GUI space has it's origin at the top left.
	//(0, 0) is the bottom left corner in pixel coordinates, 
	// and the top-right corner is (Screen.width, Screen.height).
	// result is the color of the undistorted image coordinates, whereas id.xy is the
	// distorted image coordinates
	_Result[id.xy] = float4(result, 1);
	
}// CreateImageHemisphereMirror(uint3 id : SV_DispatchThreadID)


